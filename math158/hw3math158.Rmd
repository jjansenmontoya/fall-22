---
title: "HW3 Math158"
author: "Joshua Jansen-Montoya"
date: '2022-09-21'
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---

### Problem 3.1

For the prostate data, fit a model with lpsa as the response and the other variables as predictors:   1.    Compute 90 and 95% CIs for the parameter associated with age. Using just these intervals, what could we have deduced about the p-value for age in the regression summary?
  2.    Compute and display a 95% joint confidence region for the parameters associated with age and lbph. Plot the origin on this display. The location of the origin on the display tells us the outcome of a certain hypothesis test. State that test and its outcome.
  3.    In the text, we made a permutation test corresponding to the F-test for the significance of all the predictors. Execute the permutation test corresponding to the t-test for age in this model. (Hint: {summary(g)\$coef[4,3] gets you the t-statistic you need if the model is called g.) 
  4.    Remove all the predictors that are not significant at the 5% level. Test this model against the original model. Which model is preferred?

### Answer 3.1

  1.  We can construct the following linear model and compute the following CIs for the parameter associated with age as follows,

```{r 1a}
  library(faraway)
  reg <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate)
  confint(reg, level = .90)
  confint(reg, level = .95)
  summary(reg)
```

Looking over the results from our tests, we can see that 0 is in the interval for our age variable for only the 95% interval, not for our 90% interval We can see that this agrees with our linear model which puts age significant to $\alpha$ = 0.1% (as indicated by the "."), and that the $p$-value is 8.29%.
  2.    We can plot the data as follows using the following R-code

```{r 1b}
  library(faraway)
  require(ellipse)
  lmod <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate)
  plot(ellipse(lmod,c(4,5)),type="l", main = "confidence ellipse and bands")
  points(coef(lmod)[4], coef(lmod)[5], pch=19, cex = 1.5)
  points(0,0,cex=1.5)
  abline(v = confint(lmod)[4,], lty=2)
  abline(h = confint(lmod)[5,], lty=2)
```

  3.    We can answer this problem by executing the following R code,

```{r 1c}
  library(faraway)
  reg <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate)
  regs <- summary(reg)
  nreps <- 4000
  set.seed(123)
  tstats <- numeric(nreps)
  for(i in 1:nreps){
  lmods <- lm(sample(lpsa) ~lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate)
  tstats[i] <- summary(lmods)$coef[4,3]
  }
  summary(reg)$coef[3,]
  mean(tstats > abs(regs$coef[4,3]))
```

We can see that it is quite rare that our random sample t-stats is greater than our calculate t-stat from the $p$-value we have obtained, and thus, we can conclude that our model is sufficient.
  4.    We can see from the model that the one that we can remove lcp, gleason, and pgg45 from our variables to get the following model, from which we can use anova to compare.

```{r 1d}
  library(faraway)
  reg <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate)
  reg2 <- lm(lpsa ~ lcavol + lweight + age + lbph + svi , data = prostate)
  anova(reg2, reg)
```

We can see that compared to the base model, since we have too large of a $p$-value and thus, we can accept the reduced variable model (where lcp, gleason, and pgg45 are all removed), which is our null hypothesis model.

### Problem 3.2 
  Thirty samples of cheddar cheese were analyzed for their content of acetic acid, hydrogen sulfide and lactic acid. Each sample was tasted and scored by a panel of judges and the average taste score produced.Use the cheddar data to answer the following: 
  1.    Fit a regression model with taste as the response and the three chemical contents as predictors. Identify the predictors that are statistically significant at the 5% level.
  2.    Acetic and H2S are measured on a log scale. Fit a linear model where all three predictors are measured on their original scale. Identify the predictors that are statistically significant at the 5% level for this model.
  3.    Can we use an F-test to compare these two models? Explain. Which model provides a better fit to the data? Explain your reasoning.
  4.    If H2S is increased 0.01 for the model used in (a), what change in the taste would be expected?
  5.    What is the percentage change in H2S on the original scale corresponding to an additive increase of 0.01 on the (natural) log scale?

### Answer 3.2

  1.    We can fit the following regression model with taste as the response variable and the three chemical contents as predictors as follows,

```{r 2a}
  library(faraway)
  reg <- lm(taste ~ Acetic + H2S + Lactic, data = cheddar)
  summary(reg)
```

From which we can see that only lactic and H2S are significant to a $\alpha = 0.05$ level.
  2. We can construct the desired linear regression model as follows by exponentiating the logged vectors as follows,

```{r 2b}
  library(faraway)
  regExp <- lm(taste ~ exp(Acetic) + exp(H2S) + Lactic, data = cheddar)
  summary(regExp)
```
Using the exponentiated vectors, we can see that only Lactic acid is significant for the 5% level of this model.
  3. We cannot use a F-statistic test to compare the two models, as we do not have a reduced baseline model that we are comparing to, since each model has the same number of predictor variables, just that two of them are exponentiated.
The F-statistic test is to see if we can discard any predictor variables using a baseline model, which does not apply here.
  4. If H2S is increased by 0.01, we would expect taste to increase by about 0.0169 points.
  5. The percentage change corresponding to 0.01 on the natural log scale would be an increase of 101%.

### Problem 3.3

Using the teengamb data, fit a model with gamble as the response and the other variables as predictors.
  1. Which variables are statistically significant at the 5% level?
  2. What interpretation should be given to the coefficient for sex?
  3. Fit a model with just income as a predictor and use an F-test to compare it to the full model.

### Answer 3.3

  1.  We can find which variables are statistically significant at the 5% level using the following linear model.

```{r 3a}
  library(faraway)
  ?teengamb
  reg <- lm(gamble ~ sex + status + income + verbal + gamble, data = teengamb)
  summary(reg)
```

From this, we can see that only sex and income are statistically significant at a 5% level.
  2. The coefficient for sex should be interpreted as if you are female, then according to the model you will spend 22 pounds less on gambling per year.
  3. Finally, we can complete a F-test for this for our two models as follows,

```{r 3c}
  library(faraway)
  reg <- lm(gamble ~ sex + status + income + verbal + gamble, data = teengamb)
  regInc <- lm(gamble ~ income, data = teengamb)
  anova(regInc, reg)
```

We can see from our F-statistic test that we have a $p$-value of 1.1% which is significant to our 5% level and indicates that we can ignore the null hypothesis in favor of the full model.

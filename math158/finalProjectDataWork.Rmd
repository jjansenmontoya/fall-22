---
title: "First Draft of Outputs"
author: "Joshua Jansen-Montoya"
date: '2022-11-26'
output: pdf_document
---
### Abstract

### Introduction
In our investigation, we were interested in understanding how the like count of an individuals tweets can be affected by their other actions on the app, Twitter. More specifically, given the recent relevance of Elon Musk's purchase of Twitter and his status as an inflammatory figure who has both been on the platform for an extensive amount of time, who is currently active and who garners a large amount of attention on the platform, we thought he would be a perfect case study of this phenomenon. Our motivating question for our investigation was to understand how do different metrics of Elon's tweets e.g. their retweet count, their length, and the AI (RoBERTa) generated sentiment of the tweet which attempts to provide a metric of the emotional sentiment of the tweet. We attempted to model the amount of likes that a tweet would obtain using a linear model 

### Data Transformation
Transforming the different columns in our data set to more useful numeric metrics:
Mapping each tweet to the number of characters in each tweet
```{r text -> text length}
  textLength <- vector(length=length(ElonTweets_Sentiment_10_28_22$Text))
  for (x in 1:length(ElonTweets_Sentiment_10_28_22$Text)) {
  textLength[x] = nchar(ElonTweets_Sentiment_10_28_22$Text[x])
  }
```
Mapping user name to 1 if Elon, 0 if not, location to 1 if TwitterHQ 0 if not, verified to 1 if verified, 0 if not, mentions to 1 if someone is mentioned, 0 if not
```{r user name -> 0 1}
  mentions <- ifelse(ElonTweets_Sentiment_10_28_22$mentions == "_", 0, 1)
  userName <- ifelse(ElonTweets_Sentiment_10_28_22$Username == "elonmusk", 1, 0)
  location <- ifelse(ElonTweets_Sentiment_10_28_22$location == "Twitter HQ", 1, 0)
  verified <- ifelse(ElonTweets_Sentiment_10_28_22$verified == TRUE, 1, 0)
```
Now, looking at the distribution of the different languages of Elon's Tweets, we can designate a general number for all languages that occur more than 50 times (since "en" occurs 15008 times) according to "en": 1, "de": 2, "fr": 3, "qam": 4, "qme": 5, "tl": 6, "und": 7, "zxx": 8, other: 0, giving us,
```{r language -> numeric}
  language <- ifelse(ElonTweets_Sentiment_10_28_22$language == "en", 1, ifelse(ElonTweets_Sentiment_10_28_22$language == "de", 2, ifelse(ElonTweets_Sentiment_10_28_22$language == "fr", 3, ifelse(ElonTweets_Sentiment_10_28_22$language == "qam", 4, ifelse(ElonTweets_Sentiment_10_28_22$language == "qme", 5, ifelse(ElonTweets_Sentiment_10_28_22$language == "tl", 6, ifelse(ElonTweets_Sentiment_10_28_22$language == "und", 7, ifelse(ElonTweets_Sentiment_10_28_22$language == "zxx", 8, 0))))))))
```
Now to find the time between Elons first tweet ("2010-06-04") and each of his tweets, map each date to the time since,
```{r date -> time since first tweet}
  timeSinceFirstTweet <- as.Date(as.character(ElonTweets_Sentiment_10_28_22$Date), format="%Y-%m-%d")-
                  as.Date(as.character("2010-06-04"), format="%Y-%m-%d")
  timeSinceFirstTweet<-as.numeric(timeSinceFirstTweet)
```
Now mapping the different Twitter access points Elon has for Instagram: 1, iPhone: 2, Web App: 3, Web Client: 4, other: 0.
```{r access point -> numeric}
  accessPoint <- ifelse(ElonTweets_Sentiment_10_28_22$`Twitter Access Point` == "Instagram", 1, ifelse(ElonTweets_Sentiment_10_28_22$`Twitter Access Point`== "Twitter for iPhone", 2, ifelse(ElonTweets_Sentiment_10_28_22$`Twitter Access Point`== "Twitter Web App", 3, ifelse(ElonTweets_Sentiment_10_28_22$`Twitter Access Point` == "Twitter Web Client", 4, 0))))
```
Now, we can make a mapping from our sentiments output to a number scale from -100 to 200 where -100 - 0 are designated for negative sentiments (-100 being most extreme) 0-100 for neutrality (0 being most neutral), 100-200 being positive (200 being most positive), we do that as follows,
```{r sentiment -> numeric}
  sentiments <- vector(length=length(ElonTweets_Sentiment_10_28_22$sentiment))
  for (x in 1:length(ElonTweets_Sentiment_10_28_22$sentiment)) {
    if (substring(ElonTweets_Sentiment_10_28_22$sentiment[x], 3, 5) == "pos") {
      sentiments[x] = suppressWarnings(as.numeric(substring(ElonTweets_Sentiment_10_28_22$sentiment[x], 13, 21)))*100 + 100
    } else if (substring(ElonTweets_Sentiment_10_28_22$sentiment[x], 3, 5) == "neu") {
     sentiments[x] = 100- 100*suppressWarnings(as.numeric(substring(ElonTweets_Sentiment_10_28_22$sentiment[x], 13, 21)))
   } else {
     sentiments[x] = -1* 100 * suppressWarnings(as.numeric(substring(ElonTweets_Sentiment_10_28_22$sentiment[x], 13, 21)))
  }
  }
```
Now rebinding all of our variables of interest into a new data frame to be used for our experiments,
```{r new data frame}
  tweetId <- ElonTweets_Sentiment_10_28_22$`Tweet Id`
  replyCount <- ElonTweets_Sentiment_10_28_22$`reply count`
  retweetCount <- ElonTweets_Sentiment_10_28_22$`retweet count` 
  likeCount <- ElonTweets_Sentiment_10_28_22$`like count`
  friendCount <- ElonTweets_Sentiment_10_28_22$`Friends Count`
  followerCount <- ElonTweets_Sentiment_10_28_22$`Follower Count`
  df <- data.frame(tweetId , replyCount , retweetCount , likeCount , friendCount , followerCount , sentiments , accessPoint , textLength , mentions , location , verified , userName)
  df <- na.omit(df)
  dfTrain <- df[-seq(1., NROW(df), by = 17),]
  dfPredict <- df[seq(1., NROW(df), by = 17),]
```
Generating an initial linear model to answer our question of how likeCount is affected by the other variables, we obtain that,
```{r intial linear model}
  lmod <- lm(likeCount ~ tweetId + replyCount + retweetCount + friendCount + followerCount + sentiments + accessPoint + textLength + mentions + location + verified + userName, data = dfTrain)
summary(lmod)
```
Looking at intial results, appears that we need to drop location, verified, userName, and friendCount from our predictor variables. Doing so,
```{r lmod dropping variables}
   lmod1 <- lm(likeCount ~ tweetId + replyCount + retweetCount + followerCount + sentiments + accessPoint + textLength + mentions, data = dfTrain)
  summary(lmod1)
```
Now, we can apply our step function as follows,
```{r step function}
  step(lmod1)
```
giving us the following linear model,
```{r lmod creation for our data points}
  lmod<-lm(likeCount ~ tweetId + replyCount + retweetCount + followerCount + sentiments + accessPoint + textLength + mentions, data = dfTrain)
```
Now to attempt transformations
```{r log transformation}
  lmodLog <- lm(log(likeCount) ~ tweetId + replyCount + retweetCount + followerCount + sentiments + accessPoint + textLength + mentions, data = dfTrain)
  summary(lmodLog)
```
Gives us a worse $R^2$.
```{r square root transformation}
  lmodSqrt <- lm(sqrt(likeCount) ~ tweetId + replyCount + retweetCount + followerCount + sentiments + accessPoint + textLength + mentions, data = dfTrain)
  summary(lmodSqrt)
```
ALso reduces our $R^2$. Trying 1/sqrt,
```{r neg 1/sqrt transformation}
  lmodNegSqrt <- lm(likeCount^(-1/2) ~ tweetId + replyCount + retweetCount + followerCount + sentiments + accessPoint + textLength + mentions, data = dfTrain)
  summary(lmodNegSqrt)
```
Worse $R^2$. Trying rational,
```{r rational transformation}
  lmodRat <- lm(1/likeCount ~ tweetId + replyCount + retweetCount + followerCount + sentiments + accessPoint + textLength + mentions, data = dfTrain)
  summary(lmodRat)
```
Significantly worse $R^2$ none had better $R^2$ not better significance.
no transformation should be good.
Now to calculate the condition numbers and VIF for our model,
```{r VIF & condition numbers}
  vif(lmod)
  x <- model.matrix(lmod)[,-1]
  e <- eigen(t(x) %*% x)
  sqrt(e$val[1]/e$val)
```
Our vif numbers indicate little  collinearity between our different predictor variables, but we can see that our condition numbers are super high because of the fact that it appears that although all of our variables are highly significant, some are far more significant than others, e.g. accessPoint vs. TweetID.
Now, can check our model to see if our variables meet our assumptions. First checking for normality of our variables, will check with sample of 
```{r checking normality assumptions}
  shapiro.test(sample(tweetId, 5000, replace = FALSE, prob = NULL))
  shapiro.test(sample(replyCount, 5000, replace = FALSE, prob = NULL))
  shapiro.test(sample(retweetCount, 5000, replace = FALSE, prob = NULL))
  shapiro.test(sample(sentiments, 5000, replace = FALSE, prob = NULL))
  shapiro.test(sample(accessPoint, 5000, replace = FALSE, prob = NULL))
  shapiro.test(sample(textLength, 5000, replace = FALSE, prob = NULL))
  shapiro.test(sample(mentions, 5000, replace = FALSE, prob = NULL))
  shapiro.test(sample(likeCount, 5000, replace = FALSE, prob = NULL))
```
We can see that for each of our random samples, it appears that our variable distributions are not normal and thus, some of our assumptions for our model are not met. 

Trying PLS, Partial Component, Robust to deal with non-normality of data.
Doing the Least Trimmed Squares method,
```{r lts method}
  rmse <- function(x,y) sqrt(mean((x-y)^2))
  ltsmod <- ltsreg(likeCount ~ tweetId + replyCount + retweetCount + followerCount + sentiments + accessPoint + textLength + mentions, data = dfTrain)
  pred1 <- predict(ltsmod, dfPredict)
  pred2 <- predict(lmod, dfPredict)
  rmse(pred1, dfPredict$likeCount)
  rmse(pred2, dfPredict$likeCount)
```
Notice some large distinctions in differences between our two predictions from our models, but that our original model performs better than our robust regression technique.
Now for PLS,
```{r PLS}
  library(pls)
  plsModel <- plsr(likeCount ~ tweetId + replyCount + retweetCount + followerCount + sentiments + accessPoint + textLength + mentions, data = dfTrain, scale=TRUE, validation="CV")
  pred1 <- predict(plsModel, dfPredict)
  pred2 <- predict(lmod, dfPredict)
  summary(plsModel)
  rmse(pred1, dfPredict$likeCount)
  rmse(pred2, dfPredict$likeCount)
```
The two models perform about the same but our original model seems to perform a bit better when considering our RMSE. Now Principal Component,
```{r principal component analysis}
  twitterPCA <- prcomp(dfTrain)
  summary(twitterPC)
  plot(round(twitterPC$sdev/3.098508e+17, 2)*100)
  modelPCA <- lm(likeCount ~ twitterPCA$x[,1:2], dfTrain)
  #summary(modelPCA)
```
Can find our first two PCA components for our model as follows,
```{r PCA components}
  loadings <- eigen(cov(dfTrain))$vectors
  explvar <- loadings^2
  explvar
```
Thus, we can see that for this, we only care about variables 1,2,3,4,6,7,8,9 for our first 2 PCAs.s
```{r comparing pca}
  dfPredictPCA <- data.frame(dfPredict$)
  pred1 <- predict(modelPCA, dfPredict)
  pred2 <- predict(lmod, dfPredict)
  summary(plsModel)
  rmse(pred1, dfPredict$likeCount)
  rmse(pred2, dfPredict$likeCount)
```
This tells us that all of our variance lies in one of our principal component.















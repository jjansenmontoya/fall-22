---
title: "Midterm Math 158"
author: "Joshua Jansen-Montoya"
date: '2022-11-05'
output: pdf_document
---


First, we can load the dataset as follows and then format our response variable data as defined here,
```{r format data}
  library(MASS)
  Y <- 100/(Cars93$MPG.city)
```

### Introduction
In this report, we will be attempting to construct a linear model that best uses the different variables from the Cars93 dataset to describe the response variable "MPG.city". That is, we are trying to answer the question of which variables in the Cars93 dataset are most significant and can best predict the response variable, "MPG.city". In answering this question, we will first construct a mapping for our non-numeric possible response variables to make the variables viable for linear regression. We will be using the linear models generated by R, then we will be focusing on optimizing this linear model using a step wise testing approach using stepwise regression, then utilizing Box-Cox transformations on our response variable. Finally we will be using our different techniques of evaluating our linear model to see if it truly meets our assumptions for normality using different R functions as well as to test for strenuous points that may affect our linear model. Finally, we will use different robust linear regression techniques to further evaluate the performance of our linear model.
### Data Evaluation
The data set that we are using consists of data from 93 different cars on sale in the U.S. and informs the reader as to different specs to the car which can be seen in the summary. We can look at the summary as a preliminary evaluation of our data by simply summarizing our data.
```{r qqnorm for all}
?Cars93
  summary(Cars93)
```
Looking at our results, we can see that the Make category would likely not be useful for our exploits. Similarly, for the sake of our investigation, I do not expect that Airbags, Origin, Model would be useful for our model. We can see that we have a couple of different variables that are not numeric, and thus, we will have to assign a scale to them. For the category, type, let us assign the scale (0,5) for each of Compact, Small, Midsize, Large, Sporty, and Van respectively. Similarly, we can assign (0,1) for Man.trans.available, (1,3) for Drive.Train such that Front: 0, Back: 1, 4WD: 2, and Cylinder such that rotary: -1, 3: 0, 4: 1, 5: 2, 6: 3, 8: 4. We can do this with the following code,
```{r data mappings}
  type <- ifelse(Cars93$Type == "Compact", 0, ifelse(Cars93$Type == " Small", 1, ifelse(Cars93$Type == "Midsize", 2, ifelse(Cars93$Type == "Large", 3, ifelse(Cars93$Type == "Sporty", 4, 5)))))
  trans <- ifelse(Cars93$Man.trans.avail == "Yes", 1, 0)
  drive <- ifelse(Cars93$DriveTrain == "Front", 1, ifelse(Cars93$DriveTrain == "Back", 2, 3))
  cylinders <- ifelse(Cars93$Cylinders == 3, 1, ifelse(Cars93$Cylinders == 4, 2, ifelse(Cars93$Cylinders == 5, 3, ifelse(Cars93$Cylinders == 6, 4, ifelse(Cars93$Cylinders == 8, 5, -1)))))
```
Now, using reasoning, we can think that some covariates of interest would be Horsepower, Cylinders, Type, MPG.highway, Length, Width, Weight, and Passengers. Thus, we can find boxplots for these covariates with the following code,
```{r box plots}
  boxplot(Cars93$Horsepower, xlab = "Horsepower")
  boxplot(cylinders, xlab = "Cylinders")
  boxplot(type, xlab = "Type")
  boxplot(Cars93$MPG.highway, xlab = "MPG.Highway")
  boxplot(Cars93$Horsepower, xlab ="Horsepower")
  boxplot(Cars93$Length, xlab = "Length")
  boxplot(Cars93$Width, xlab = "Width")
  boxplot(Cars93$Passengers, xlab = "Passengers")
  boxplot(drive, xlab = "Drive")
  boxplot(trans, xlab = "Transmission")
  boxplot(Cars93$Weight, xlab = "Weight")
```
From this, we can see that our values are reasonably well distributed and have fairly few outliers. Now, we can graph a few of our variables of interest versus our desired response variable as follows,
```{r response variable}
  plot(Cars93$MPG.highway, Y)
  plot(type, Y)
  plot(cylinders, Y)
  plot(Cars93$EngineSize, Y)
  plot(Cars93$Weight, Y)
```
We can note that there seem to be linear relationships between $\frac{100}{City MPG}$ and MPG.Highway, Weight and Engine size, although our other two variables are a bit more difficult to decipher, so we anticipate that these values may not be present in our final linear model. Using these plots, it seems as though it would be appropriate to use a linear model to describe our response variable using our predictor variables as desired. 

### Methodology
For our analysis, we will first construct a standard linear model using our data set given variables that we have deemed to be significant through our exploratory analysis. Then, we will use the step() function for our model selection. From this, we will be able to determine the best possible set of predictor variables for our linear model. After generating such a linear model, we can then apply the Box-Cox test to see whether or not we should apply some kind of transformation for our response variable to further improve our model. Once we have our model, we will check our predictor variables for normality using the Wilkes-Shapiro Test and QQ-Plot, and use VIF() and calculate the condition numbers to check for collinearity in our predictor variables to test our assumptions for our linear model to determine whether or not our linear model is appropriate, or if a different linear model would be appropriate. Finally, we will then fit a linear model using the Huber method, Least absolution deviations, and Least trimmed squares to determine if our robust regression techniques deviate greatly from our standard linear model. 

### Analysis
First, we will construct the linear model with all of the predictor variables in the dataset that we deemed to possibly be significant in our exporatory analysis,
```{r first lmod}
  HighwayMPG <-Cars93$MPG.highway
  Price <- Cars93$Price
  EngineSize <-  Cars93$EngineSize
  HorsePower <- Cars93$Horsepower
  RPM <- Cars93$RPM
  REV <- Cars93$Rev.per.mile
  Passengers <- Cars93$Passengers
  Length <- Cars93$Length
  Wheelbase <- Cars93$Wheelbase
  Width <- Cars93$Width
  TurnCircle <- Cars93$Turn.circle
  RearSeatRoom <- Cars93$Rear.seat.room
  Luggage <- Cars93$Luggage.room 
  Weight <- Cars93$Weight
  FuelTank <- Cars93$Fuel.tank.capacity
  dataFrame <- data.frame(Y, HighwayMPG, Price, EngineSize, HorsePower, RPM, REV, Passengers, Length, Wheelbase, Width, TurnCircle, RearSeatRoom, Luggage, Weight, FuelTank)
  dataFrame <- na.omit(dataFrame)
  lmod <- lm(Y ~ HighwayMPG+ Price+ EngineSize+ HorsePower+ RPM+ REV+ Passengers+ Length+ Wheelbase+ Width+ TurnCircle+ RearSeatRoom+ Luggage+ Weight + FuelTank, data = dataFrame)
  summary(lmod)
```
From there, we can now use the step function to determine the optimal linear model,
```{r step}
  step(lmod)
```
Now, we can see that as a result of our step function, we find that the following linear model is our ideal model which has HighwayMPG, Price, EngineSize, FuelTankCapacity, and Width as our predictor variables. Thus, we can construct such a linear model as follows,
```{r summary of lmod}
  lmod <- lm(formula = Y ~ HighwayMPG + Price + EngineSize + Width + FuelTank, data = dataFrame)
  summary(lmod)
```
Now, looking at these results, we can see that we have a high $R^2$, as well as a statsitically signficant p-value which make us feel as though we there is some legitimacy to this linear model. We can also note that while our $R^2$ did go down from our original model, we can see that our adjusted $R^2$ actually decreased which thus indicates to us that we are better off with fewer predictor variables since we our calculations indicated that we did not need so many variables.
Now, we can implement our Box-Cox test to determine whether or not we should apply a transformation to our predictor variable. 
```{r box cox}
  bc <- boxcox(lmod)
  bc$x[which.max(bc$y)]
```
From this, we can see that a transformation of $x^{\frac{2}{3}}$ would be an ideal tranformation (rounding to a clean rational number). Thus applying that, we obtain, that,
```{r box-cox transformation}
  lmodBoxCox <- lm(Y^(2/3) ~ HighwayMPG + Price + EngineSize + Width + FuelTank, data = dataFrame)
  summary(lmodBoxCox)
```
Which we can makes Width and FuelTank more significant, but Price less signficant. We can also see that both our adjusted $R^2$ and our regular $R^2$ both increased which gives further legitimacy to the transformation that we made. Looking at our parameters, we can see that this model gives us a linear model of the form of $Y = 2.835 + -0.0428x_1 + 0.00247x_2 + 0.0922x_3 + 0.0103x_4 + 0.01185x_5$, which indicates to us that if all other variables were held constant, if we have a 1MPG increase in the HighwayMPG, we would expect to be able to consume 0.0428 gallons less of gasoline over 100 miles within the city, that if we pay one thousand dollars more for our car, we would expect to consume 0.002471 gallons more of gas per 100 miles in the city, if we have an 1 Liter increase in the engine size, we can expect to consume 0.0922 gallons more of gas per 100 miles in the city, if we have a 1 inch increase in the width of our car, then we would expect to consume 0.0103 gallons more of gas per 100 miles in the city, and if we have an increase in the capacity of our fuel tank of 1 gallon, we would expect to have an increase of 0.0112 gallons of gas consumed per 100 miles in the city. 

#### Evaluation of Model
Now that we have produced our model, we can now perform our regression diagnostics. We will first apply the Wilkes-Shapiro test on each of our predictor variables,
```{r Wilk-Shapiro}
  ?shapiro.test
  shapiro.test(HighwayMPG)
  shapiro.test(Price)
  shapiro.test(EngineSize)
  shapiro.test(Width)
  shapiro.test(FuelTank)
  shapiro.test(Y)
```
Looking at our results, we can see that for a value of $\alpha = 0.05$, the only one of our predictor variables that does not differ from the normal distribution in a statstically significant way is our FuelTank predictor. Similarly, our response variable does meeting our assumptions of normality. Visually, we can see from the qqplots of our non-normally distributed predictor variables that,
```{r qqnorm plots}
  qqnorm(HighwayMPG)
  qqline(HighwayMPG)
  qqnorm(Price)
  qqline(Price)
  qqnorm(EngineSize)
  qqline(EngineSize)
  qqnorm(Width)
  qqline(Width)
```
We can see that these qqplots corroborate with our worries about the normality of our predictor variables. Thus, we can see that our assumption of normality may not be met for our linear model.
Now, looking for leverage points, we can conduct our search for these points as follows,
```{r leverage points}
  hatvalues(lmodBoxCox) > 2*mean(hatvalues(lmodBoxCox))
```
Thus, we can see that we find high leverage points at indices 5, 8, 18, 39, 42, 59, 60, and 80. Now, checking for outliers, we find that,
```{r outliers}
  rstandard(lmodBoxCox)[abs(rstandard(lmodBoxCox))>2]
```
Which gives us our point 58 as an outlier. Finally, we can look for highly influential points using the following code,
```{r influential points}
  cooks.distance(lmodBoxCox)>4/length(cooks.distance(lmodBoxCox))
```
Which we can see gives us points 5, 10, 39, 48, 58, 59, and 91 all as highly influential points. Thus, we can now refit our model removing point 58 and then consider our resulting model as follows,
```{r removed points model}
  removedDataFrame <- dataFrame[-c(58), ]
  lmodRemovedPoint <- lm(Y^(2/3) ~ HighwayMPG + Price + EngineSize + Width + FuelTank, data = dataFrame)
  summary(lmodRemovedPoint)
```
Which we can see does not do anything to the estimated coefficients, nor to our $R^2$ values or F-statistic. Thus, we can conclude that our analysis does not change really when we remove this outlier point. 
Now, checking for collinearity with the VIFs and condition numbers for our model, we find that,
```{r Condition numbers}
  X <- model.matrix(lmodBoxCox)[,-1]
  e <- eigen(t(X)%*%X)
  sqrt(e$val[1]/e$val)
```
Looking at these values, we can see that there appears to be a high level of collinearity within our set of predictor variables as indicated by our large condition numbers. We can now check the VIF as well,
```{r VIF}
  require(faraway)
  vif(X)
```
Specifically, we can see that there appears to be a high level of collinearity specifically wiht our EngineSize, Width, and FuelTank variables. We can check for this as follows,
```{r correlation checks}
  cor(EngineSize, Width)
  cor(EngineSize, FuelTank)
  cor(FuelTank, Width)
```
Which we can see an especially high level of correlation between these three variables. Finally, to quell some of these issues, we can now fit a model using the Huber method, least absolute deviations, and the least trimmed squares methods. Computing those here,
```{r other linear model methods}
  huberMethod <- rlm(Y^(2/3) ~ HighwayMPG + Price + EngineSize + Width + FuelTank, data = dataFrame)
  require(quantreg)
  l1Mod <- rq(Y^(2/3) ~ HighwayMPG + Price + EngineSize + Width + FuelTank, data = dataFrame)
  ltsmod <- ltsreg(Y^(2/3) ~ HighwayMPG + Price + EngineSize + Width + FuelTank, data = dataFrame)
  print("huber method")
  summary(huberMethod)
  print("L1Mod")
  summary(l1Mod)
  print("ltsMod")
  coef(ltsmod)
```
  Looking first at the Huber method, we can see that our results were pretty similar withour original linear model, as indicated by the similar coefficients and approximately similar t-values calculated for each of our predictor variables. We can see that this corroborates well also with our least absolute deviations model and our original model, as each of calculated coefficients for our original linear model lies within the bounds of our LAD linear model. However, we can note some discrepencies between our least trimmed squares method, specifically with the discrpencies between the coefficients for our intercept, Price, EngineSize, and Width. 
  
### Conclusion
From our analysis, we were able to fit an ordinary linear model explaining our desired response variable, the number of gallons that a car consumes to cover 100 miles within the city. Using the step function, we found that the most relevant predictive variables were HighwayMPG, Price, EngineSize, Width, and FuelTank Capacity. However, in our evaluation of the model, we did find that some of our assumptions for our linear model, such as normality, were not met by all of our variables which casts doubt into whether or not we can use our linear model. However, we can note that when we applied our robust regression techniques, we generally found consensus between our ordinary linear model and our robust techniques (Huber and LAD). Therefore, there is some indication that we can trust our linear model, or that we can simply use our other robust techniques, as we did find that the outlier points did not affect our model too intensely as indicated by the fact that when we removed the point, we did not have much change to our model. It is important to be weary of this model however and further investigation is likely necessary due to our the failure of normality for our predictor variables which is a key assumption for utilizing a linear model. It is also interesting to consider that a lot of the variables that we initially believed to likely be interesting and important to our analysis were not such (e.g. Weight and Type), and that the EngineSize was one of the most important predicting variables when it came to predicting the fuel efficiency.
  
